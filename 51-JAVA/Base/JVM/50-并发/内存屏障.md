## 前言
    之前读了关于顺序一致性和缓存一致性讨论的文章，感觉豁然开朗。对linux内核中出现的种种同步和屏障，想做一点总结。

## 缓存一致性
    之前一直认为linux中很多东西是用来保证缓存一致性的，其实不是。缓存一致性绝大部分是靠硬件机制实现的，只有在带lock前缀的指令执行时才与cache有一点关系。（这话说得绝对，但我目前看来就是这样）我们更多的时候是为了保证顺序一致性。
    所谓缓存一致性，就是在多处理器系统中，每个cpu都有自己的L1 cache。很可能两个不同cpu的L1 cache中缓存的是同一片内存的内容，如果一个cpu更改了自己被缓存的内容，它要保证另一个cpu读这块数据时也要读到这个最新的。不过你不要担心， 这个复杂的工作完全是由硬件来完成的，通过实现一种MESI协议，硬件可以轻松的完成缓存一致性的工作。不要说一个读一个写，就是多个同时写都没问题。一 个cpu读时总能读入最新的数据，不管它是在自己的cache中，还是在其它cpu的cache中，还是在内存中，这就是缓存一致性。

## 顺序一致性
    所谓顺序一致性，说的则是与缓存一致性完全不同的概念，虽然它们都是处理器发展的产物。因为编译器的技术不断发展，它可能为了优化你的代码，而将某些操作 的顺序更改执行。处理器中也早就有了多发射、乱序执行的概念。这样的结果，就是实际执行的指令顺序会与编程时代码的执行顺序略有不同。这在单处理器下当然 没什么，毕竟只要自己的代码不过问，就没人过问，编译器和处理器就是在保证自己的代码发现不了的情况下打乱执行顺序的。但多处理器不是这样，可能一个处理 器上指令的完成顺序，会对其它处理器上执行的代码造成很大影响。所以就有了顺序一致性的概念，即保证一个处理器上线程的执行顺序，在其它的处理器上的线程 看来，都是一样的。这个问题的解决不是光靠处理器或者编译器就能解决的，需要软件的干预。

## 内存屏障
    软件干预的方式也非常简单，那就是插入内存屏障(memory barrier)。其实内存屏障这个词，是由搞处理器的人造的，弄得我们很不好理解。内存屏障，很容易让我们串到缓存一致性去，乃至怀疑是否这样做才能让 其它cpu看到被修改过的cache，这样想就错了。所谓内存屏障，从处理器角度来说，是用来串行化读写操作的，从软件角度来讲，就是用来解决顺序一致性 问题的。编译器不是要打乱代码执行顺序吗，处理器不是要乱序执行吗，你插入一个内存屏障，就相当于告诉编译器，屏障前后的指令顺序不能颠倒，告诉处理器， 只有等屏障前的指令执行完了，屏障后的指令才能开始执行。当然，内存屏障能阻挡编译器乱来，但处理器还是有办法。处理器中不是有多发射、乱序执行、顺序完 成的概念吗，它在内存屏障时只要保证前面指令的读写操作，一定在后面指令的读写操作完成之前完成，就可以了。所以内存屏障才会对应有读屏障、写屏障和读写 屏障三类。如x86之前保证写操作都是顺序完成的，所以不需要写屏障，但现在也有部分ia32处理器的写操作变成乱序完成，所以也需要写屏障。
    其实，除了专门的读写屏障指令，还有很多指令的执行是带有读写屏障功能的，比如带lock前缀的指令。在专门的读写屏障指令出现之前，linux就是靠lock熬过来的。
    至于在那里插入读写屏障，要视软件的需求而定。读写屏障无法完全实现顺序一致性，但多处理器上的线程也不会一直盯着你的执行顺序看，只要保证在它看过来 的时候，认为你符合顺序一致性，执行不会出现你代码中没有预料到的情况。所谓预料外的情况，举例而言，你的线程是先给变量a赋值，再给变量b赋值，结果别 的处理器上运行的线程看过来，发现b赋值了，a却没有赋值，（注意这种不一致不是由缓存不一致造成的，而是处理器写操作完成的顺序不一致造成的），这时就 要在a赋值与b赋值之间，加一个写屏障。

## 多处理器间同步
    有了SMP之后，线程就开始同时在多个处理器上运行。只要是线程就有通信和同步的要求。幸好SMP系统是共享内存的，也就是所有处理器看到的内存内容都 一样，虽然有独立的L1 cache，但还是由硬件完成了缓存一致性处理的问题。那不同处理器上的线程要访问同一数据，需要临界区，需要同步。靠什么同步？之前在UP系统中，我们 上靠信号量，下靠关中断和读修改写指令。现在在SMP系统中，关中断已经废了，虽然为了同步同一处理器上的线程还是需要的，但只靠它已经不行了。读修改写 指令？也不行了。在你指令中读操作完成写操作还没进行时，就可能有另外的处理器进行了读操作或者写操作。缓存一致性协议是先进，但还没有先进到预测这条读 操作是哪种指令发出来的。所以x86又发明了带lock前缀的指令。在此指令执行时，会将所有包含指令中读写地址的cache line失效，并锁定内存总线。这样别的处理器要想对同样的地址或者同一个cache line上的地址读写，既无法从cache中进行（cache中相关line已经失效了），也无法从内存总线上进行（整个内存总线都锁了），终于达到了原 子性执行的目的。当然，从P6处理器开始，如果带lock前缀指令 要访问的地址本来就在cache中，就无需锁内存总线，也能完成原子性操作了（虽然我怀疑这是因为加了多处理器内部公共的L2 cache的缘故）。因为会锁内存总线，所以带lock前缀指令执行前，也会先将未完成的读写操作完成，也起到内存屏障的功能。
    现在多处理器间线程的同步，上用自旋锁，下用这种带了lock前缀的读修改写指令。当然，实际的同步还有加上禁止本处理器任务调度的，有加上任务关中断的，还会在外面加上信号量的外衣。linux中对这种自旋锁的实现，已历经四代发展，变得愈发高效强大。


内存屏障的实现

[cpp] view plaincopyprint?
#ifdef CONFIG_SMP  
#define smp_mb()    mb()  
#define smp_rmb()   rmb()  
#define smp_wmb()   wmb()  
#else  
#define smp_mb()    barrier()  
#define smp_rmb()   barrier()  
#define smp_wmb()   barrier()  
#endif  
CONFIG_SMP就是用来支持多处理器的。如果是UP(uniprocessor)系统，就会翻译成barrier()。

[cpp] view plaincopyprint?
#define barrier() __asm__ __volatile__("": : :"memory")  
barrier()的作用，就是告诉编译器，内存的变量值都改变了，之前存在寄存器里的变量副本无效，要访问变量还需再访问内存。这样做足以满足UP中所有的内存屏障。

[cpp] view plaincopyprint?
#ifdef CONFIG_X86_32  
/* 
 * Some non-Intel clones support out of order store. wmb() ceases to be a 
 * nop for these. 
 */  
#define mb() alternative("lock; addl $0,0(%%esp)", "mfence", X86_FEATURE_XMM2)  
#define rmb() alternative("lock; addl $0,0(%%esp)", "lfence", X86_FEATURE_XMM2)  
#define wmb() alternative("lock; addl $0,0(%%esp)", "sfence", X86_FEATURE_XMM)  
#else  
#define mb()    asm volatile("mfence":::"memory")  
#define rmb()   asm volatile("lfence":::"memory")  
#define wmb()   asm volatile("sfence" ::: "memory")  
#endif  
如 果是SMP系统，内存屏障就会翻译成对应的mb()、rmb()和wmb()。这里CONFIG_X86_32的意思是说这是一个32位x86系统，否则 就是64位的x86系统。现在的linux内核将32位x86和64位x86融合在同一个x86目录，所以需要增加这个配置选项。

可以看到，如果是64位x86，肯定有mfence、lfence和sfence三条指令，而32位的x86系统则不一定，所以需要进一步查看cpu是否支持这三条新的指令，不行则用加锁的方式来增加内存屏障。
SFENCE,LFENCE,MFENCE指令提供了高效的方式来保证读写内存的排序,这种操作发生在产生弱排序数据的程序和读取这个数据的程序之间。 
   SFENCE——串行化发生在SFENCE指令之前的写操作但是不影响读操作。 
   LFENCE——串行化发生在SFENCE指令之前的读操作但是不影响写操作。 
   MFENCE——串行化发生在MFENCE指令之前的读写操作。 
sfence:在sfence指令前的写操作当必须在sfence指令后的写操作前完成。 
lfence：在lfence指令前的读操作当必须在lfence指令后的读操作前完成。 
mfence：在mfence指令前的读写操作当必须在mfence指令后的读写操作前完成。 

至于带lock的内存操作，会在锁内存总线之前，就把之前的读写操作结束，功能相当于mfence，当然执行效率上要差一些。

说起来，现在写点底层代码真不容易，既要注意SMP问题，又要注意cpu乱序读写问题，还要注意cache问题，还有设备DMA问题，等等。